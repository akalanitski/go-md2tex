package tokenizer

// TokenType is a byte ID for each token type.
type TokenType byte

const (
	TOKEN_ILLEGAL TokenType = iota
	TOKEN_WORD
	TOKEN_NUMBER
	TOKEN_SPACE
	TOKEN_DOT
	TOKEN_COMMA
	TOKEN_ASTERISK
	TOKEN_UNDERSCORE
	TOKEN_SQBRACKET_BEGIN
	TOKEN_SQBRACKET_END
	TOKEN_PHARENTHESIS_BEGIN
	TOKEN_PHARENTHESIS_END
	TOKEN_BACKTICK
	TOKEN_AMPERSAND
	TOKEN_PROCENT
	TOKEN_DOLLAR
	TOKEN_SHARP
	TOKEN_CBRACKET_BEGIN
	TOKEN_CBRACKET_END
	TOKEN_TILDA
	TOKEN_CARAT
	TOKEN_BACKSLASH
	TOKEN_SLASH
	TOKEN_SEMICOLON
	TOKEN_DOUBLE_QUOTE
	TOKEN_SINGLE_QUOTE
	TOKEN_HYPHEN
	TOKEN_NEWLINE
)

// tokenTypeName is a map with names of each toke for debug purpose
var TokenTypeNames = map[TokenType]string{
	TOKEN_WORD:               "WORD",
	TOKEN_NUMBER:             "INT",
	TOKEN_SPACE:              "SPACE",
	TOKEN_DOT:                "DOT",
	TOKEN_COMMA:              "COMMA",
	TOKEN_NEWLINE:            "NEWLINE",
	TOKEN_ASTERISK:           "ASTERISK",
	TOKEN_UNDERSCORE:         "UNDERSCORE",
	TOKEN_SQBRACKET_BEGIN:    "SQBRACKET_BEGIN",
	TOKEN_SQBRACKET_END:      "SQBRACKET_END",
	TOKEN_PHARENTHESIS_BEGIN: "PHARENTHESIS_BEGIN",
	TOKEN_PHARENTHESIS_END:   "PHARENTHESIS_END",
	TOKEN_BACKTICK:           "BACKTICK",
	TOKEN_AMPERSAND:          "TOKEN_AMPERSAND",
	TOKEN_PROCENT:            "PROCENT",
	TOKEN_DOLLAR:             "DOLLAR",
	TOKEN_SHARP:              "SHARP",
	TOKEN_CBRACKET_BEGIN:     "CBRACKET_BEGIN",
	TOKEN_CBRACKET_END:       "CBRACKET_END",
	TOKEN_TILDA:              "TILDA",
	TOKEN_CARAT:              "CARAT",
	TOKEN_BACKSLASH:          "BACKSLASH",
	TOKEN_SLASH:              "SLASH",
	TOKEN_SEMICOLON:          "SEMICOLON",
	TOKEN_DOUBLE_QUOTE:       "DOUBLE_QUOTE",
	TOKEN_SINGLE_QUOTE:       "SINGLE_QUOTE",
	TOKEN_HYPHEN:             "HYPHEN",
	TOKEN_ILLEGAL:            "ILLEGAL",
}
